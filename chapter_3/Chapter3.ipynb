{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = 5\n"
     ]
    }
   ],
   "source": [
    "# Load tensorflow package\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "\n",
    "\n",
    "d = tf.multiply(a,b)\n",
    "e = tf.add(c,b)\n",
    "f = tf.subtract(d,e)\n",
    "\n",
    "#creating  a Session and Running it.\n",
    "#A session object is the part of the TF API that communicates between Python objects \n",
    "sess = tf.Session()\n",
    "outs = sess.run(f)\n",
    "sess.close()\n",
    "print(\"outs = {}\".format(outs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Constructing and Managing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7faedc4eee10>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7faedc4ee8d0>\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_default_graph())\n",
    "g = tf.Graph()\n",
    "p = tf.constant(5)\n",
    "print(g)\n",
    "print(a.graph is g) # To check for default graph\n",
    "print(a.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The \"with\" statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The with statement is used to wrap the execution of a block with\n",
    "methods defined by a context manager—an object that has the special\n",
    "method functions .__enter__() to set up a block of code\n",
    "and .__exit__() to exit the block. \n",
    "* Opening a session using the with clauser will ensure the session is automatically closed once all computations are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.get_default_graph()\n",
    "g2 = tf.Graph()\n",
    "\n",
    "print(g1 is tf.get_default_graph())\n",
    "\n",
    "with g2.as_default():\n",
    "    print(g1 is tf.get_default_graph())\n",
    "    \n",
    "print(g1 is tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, outs = sess.run(f)  we request one specific node (node f) by passing the\n",
    "variable it was assigned to as an argument to the sess.run() method. This argument\n",
    "is called fetches, corresponding to the elements of the graph we wish to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = [5, 2, 3, 10, 5, 5]\n",
      "<type 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    fetches = [a,b,c,d,e,f]\n",
    "    outs =sess.run(fetches)\n",
    "    \n",
    "print(\"outs = {}\".format(outs))\n",
    "print(type(outs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowing Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we construct a node in the graph, like we did with tf.add(), we are actually\n",
    "creating an operation instance. These operations do not produce actual values until\n",
    "the graph is executed, but rather reference their to-be-computed result as a handle\n",
    "that can be passed on—flow—to another node. These handles, which we can think of\n",
    "as the edges in our graph, are referred to as Tensor objects, and this is where the\n",
    "name TensorFlow originates from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_4:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant(4.0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic units of data that pass through a graph are numerical, Boolean, or string elements.\n",
    "We can ecplicitly choose what data type we want to work with by specifying it when\n",
    " we create the Tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_5:0\", shape=(), dtype=float64)\n",
      "<dtype: 'float64'>\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant(4.0, dtype=tf.float64)\n",
    "print(c)\n",
    "print(c.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_6:0\", shape=(), dtype=string)\n",
      "<dtype: 'string'>\n"
     ]
    }
   ],
   "source": [
    "v = tf.constant(\"4.0\", dtype=tf.string)\n",
    "print(v)\n",
    "print(v.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to make sure the data types match throughout the graph - \n",
    "performing an operation with nonmatching data types will result in an exception.To change\n",
    "the data type setting of a Tensor object, we can use the tf.cast() operation, passing the relevant \n",
    "Tensor and the new data type of interest as first and second arguments respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "<dtype: 'int64'>\n",
      "<dtype: 'string'>\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1,2,3], name='x', dtype=tf.float32)\n",
    "print(x.dtype)\n",
    "x = tf.cast(x,tf.int64)\n",
    "print(x.dtype)\n",
    "x = tf.cast(x,tf.string)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tensor Arrays and Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python List input: (2, 3)\n",
      "3d NumPy array input: (2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "c = tf.constant([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(\"Python List input: {}\".format(c.get_shape()))\n",
    "\n",
    "c = tf.constant(np.array\n",
    "                ([\n",
    "                    [[1,2,3], \n",
    "                    [4,5,6]], \n",
    "                    [[1,1,1], \n",
    "                    [2,2,2]]\n",
    "                            ])) \n",
    "print(\"3d NumPy array input: {}\".format(c.get_shape()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_shape() method returns the shape of the tensor as a tuple of integers. The number of integers corresponds to the \n",
    "number of dimensions of the tensor and each integer is the numter of array entries along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of 'c' : \n",
      " [ 0.  1.  2.  3.  4.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "c = tf.linspace(0.0, 4.0, 5)\n",
    "print(\"The content of 'c' : \\n {} \\n\".format(c.eval()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence generator tf.linspace(a,b,n) where n evenly spaced values from a to b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.InteractiveSession() allows the replacement of tf.Session() without the need of assigning a variable to hold the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a Tensor storing a matrix A and another storing a vector x, and we wish to compute the matrix product of the\n",
    "two:\n",
    "    Ax = b\n",
    "This can be achieved by tf.matmul(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Tensor object has an identifying name. This name is an intrinsic string name, not to be confused with the name of the\n",
    "variable. As with dtype, we can use the .name atrribute to see the name of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:0\n",
      "c_1:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    c1 = tf.constant(4, dtype=tf.float64, name = \"c\")\n",
    "    c2 = tf.constant(4, dtype=tf.int32, name = \"c\")\n",
    "    \n",
    "print(c1.name)\n",
    "print(c2.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects residing within the same graph cannot have the same name - TensorFlow forbids it. As a consequence, it will\n",
    "automatically add an underscore/space and a number to distinguish the two. However, both objects have the same name \n",
    "when they are associated with different graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases when dealing with a large, complicated graph, we would like to create some node grouping to make it easier\n",
    "to follow and manage. This is done by using tf.name_scope(\"prefix\") together with the useful with clause again:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:0\n",
      "prefix_name/c:0\n",
      "prefix_name/c_1:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    c1 = tf.constant(4, dtype=tf.float64, name = \"c\")\n",
    "    with tf.name_scope(\"prefix_name\"):\n",
    "        c2 = tf.constant(4, dtype=tf.int32, name = \"c\")\n",
    "        c3 = tf.constant(4, dtype=tf.float64, name = \"c\")\n",
    "    \n",
    "    print(c1.name)\n",
    "    print(c2.name)\n",
    "    print(c3.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have grouped objects contained in variable c2 and c3 under the scope prefix_name, which show up as \n",
    "a prefix in their names. This is useful for visualization of the graph structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables, Placeholders, and Simple Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TensorFlow uses special objects called Variables. Unlike other Tensor objects that are\n",
    "\"refilled\" with data each time we run the session. Variable can maintain a fixed\n",
    "state in the graph. Variables can be used as input for other operations in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable usages are of two stages.  First,as tf.Variable() function in order to create a Variable and define what it will be initialized with.\n",
    "Second, as tf.global_variables_initializer() method for memory allocation for the Variable and sets its initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre run: \n",
      "\n",
      "\n",
      "post run: \n",
      "[[-0.82054317  0.61283463 -0.57423735 -0.84147424  0.30096558]]\n"
     ]
    }
   ],
   "source": [
    "init_val = tf.random_normal((1,5),0,1)\n",
    "var = tf.Variable(init_val, name='var')\n",
    "print(\"pre run: \\n\".format(var))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    post_var = sess.run(var)\n",
    "    \n",
    "print(\"\\npost run: \\n{}\".format(post_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders as the name implies can be thought as empty Variables that will be filled \n",
    "with data later on. We use them by first constructing graph and only when it is \n",
    "executed feeding them with the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder have optional shape argument. If a shape is not fed or passed as None,\n",
    "then the placeholder can be fed with data of any size. It is common to use None, for\n",
    "the dimension of a matrix that corresponds to the number of samples(usually rows),\n",
    "while having the length of the features(usually columns) fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ph = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we define a placeholder, input  values must be fed to it or else exception will be thrown.\n",
    "The input data is passed to the session.run() method as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.run(s, feed_dict={x: X_data, w: w_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These input are matrix-multiplied to create a five-unit vector xw and added with a constant vector b \n",
    "filled with the value -1. Finally, the variable s takes the maximum value of that \n",
    "vector by using tf.reduce_max() operation. The word reduce is used because we are\n",
    "reducing a five-unit vector to a single scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = -1.37360680103302\n"
     ]
    }
   ],
   "source": [
    "x_data = np.random.randn(5,10)\n",
    "w_data = np.random.randn(10,1)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=(5,10))\n",
    "    w = tf.placeholder(tf.float32, shape=(10,1))\n",
    "    b = tf.fill((5,1),-1.)\n",
    "    xw = tf.matmul(x,w)\n",
    "    xwb = xw + b\n",
    "    s = tf.reduce_max(xwb)\n",
    "    with tf.Session() as sess:\n",
    "        outs = sess.run(s,feed_dict={x: x_data, w: w_data})\n",
    "        \n",
    "print(\"outs = {}\".format(outs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization process of a simple regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by describing our regression model:\n",
    "\n",
    "f(xi) = wTxi + b\n",
    "\n",
    "yi = f(xi) + εi\n",
    "\n",
    "f(xi)is assumed to be a linear combination of some input data xi, with a set of weight w \n",
    "and an intercept b. Our target output yi is a noisy version of f(xi) after being summed with \n",
    "Gaussian noise εi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create placeholders for our input and output data and Variable for our weights and intercept(bias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y_true = tf.placeholder(tf.float32, shape=None)\n",
    "w = tf.Variable([[0,0,0]], dtype=tf.float32,name=\"weights\")\n",
    "b = tf.Variable(0,dtype=tf.float32,name=\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predicted output y_pred is the result of matrix multiplication of our input container x\n",
    "and our weight w plus a bias term b:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred =tf.matmul(w,tf.transpose(x)) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definining  a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To capture the discrepancy between our model's predictions and the observed target we need a \n",
    "measure reflecting \"distance.\" This distance is otfen called loss function or objective function. \n",
    "Then goal is to optimize the model by finding set of parameters (weights and bias ) that minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE and cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE stands for mean squared error, which is the average squared  distance between the real target \n",
    "and the predicted model also called residuals.\n",
    "In our linear regression example, we take the difference between the true target and predicted. We use\n",
    "tf.square() to compute the square of the difference vector. We then averaged the result using tf.reduce_mean() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss = tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy is another common loss use for categorical data. It uses the softmax classifier. Cross entropy is a measure of similarity between two distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "#loss = tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Optimization involves updating set of weights iteratively in a way that decreases the\n",
    "loss over time. The most commonly used approach is gradient descent, where\n",
    "the loss's gradient with respect to the set of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally is it makes sense to calculate the gradient for the entire set of \n",
    "samples in order to benefit from the maximum amount of available information. This method,\n",
    "however, has some shortcomings which can be very slow and therefore \n",
    "intractable when the dataset requires more memory than is available.\n",
    "\n",
    "The general applicable technique is the stochastic gradient descent(SGD)\n",
    "where instead of feeding the entire dataset to the algorithm for the \n",
    "computation of each step, a subset of the data sampled sequentially.\n",
    "\n",
    "The number of samples ranges from one sample at a time to a few hundred\n",
    ", but the common sizes are between 50 - 500(commonly referred to as mini-batches).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow the gradient descent it computed automatically. An important parameter to\n",
    "set is the algorithms' learning rate, this determines how aggressive each\n",
    "update iteration will be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: In this problem we are interested in retrieving a set of \n",
    "    weights w and a bias term b, assuming our target value is a linear\n",
    "    combination of some input vector x, with an additional Gaussian noise Ei added to each sample.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[ 0.30403277,  0.49297577,  0.08719987]], dtype=float32), -0.18051079]\n",
      "5 [array([[ 0.29727903,  0.4986017 ,  0.10117673]], dtype=float32), -0.20137946]\n",
      "10 [array([[ 0.29727903,  0.4986017 ,  0.10117673]], dtype=float32), -0.20137946]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#== Create data and simulate results\n",
    "x_data = np.random.randn(2000,3)\n",
    "w_real = [0.3,0.5,0.1]\n",
    "b_real = -0.2\n",
    "\n",
    "noise = np.random.randn(1,2000)*0.1\n",
    "y_data = np.matmul(w_real, x_data.T) + b_real + noise\n",
    "\n",
    "NUM_STEPS = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "wb_ = []\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None,3])\n",
    "    y_true = tf.placeholder(tf.float32,shape=None)\n",
    "   #y_pred = tf.matmul(w,tf.transpose(x)) + b\n",
    "    \n",
    "    with tf.name_scope('inference') as scope:\n",
    "        w = tf.Variable([[0,0,0]],dtype=tf.float32,name='weights')\n",
    "        b = tf.Variable(0,dtype=tf.float32,name='bias')\n",
    "        y_pred = tf.matmul(w,tf.transpose(x)) + b\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "    with tf.name_scope('train') as scope:\n",
    "        learning_rate = 0.5\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(loss) \n",
    "        \n",
    "        \n",
    "    # Before starting, initialize the variables. We will 'run' this first.\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for step in range(NUM_STEPS):\n",
    "            sess.run(train,{x: x_data, y_true: y_data})\n",
    "            if (step % 5 == 0):\n",
    "                print(step, sess.run([w,b]))\n",
    "                wb_.append(sess.run([w,b]))\n",
    "        print(10, sess.run([w,b]))                                     \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result the estimated weights and bias are w =[0.3,0.5,0.098] and b = -0.198.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Logistic regression. We wish to retrieve the weights and bias components in a simulated data setting, this time in a\n",
    "    logistic regression framework. In this case, the linear component wTx + b is the input of a nonlinear functiion called \n",
    "    the logistic function. What is effectively does is squash the values of the linear part into the interval [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function we are using is called sigmoid function. We generate our samples by using the same set of weights and biases as in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    918\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[1;32m--> 919\u001b[1;33m                                                     allow_operation=False)\n\u001b[0m\u001b[0;32m    920\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2404\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2405\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2483\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2484\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2485\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-7751db1422e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_STEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    920\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[1;32m--> 922\u001b[1;33m                             + e.args[0])\n\u001b[0m\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "N = 2000\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# === Create data and simulate results ===\n",
    "x_data = np.random.randn(N,3)\n",
    "w_real = [0.3,0.5,0.1]\n",
    "b_real = -0.2\n",
    "\n",
    "wxb = np.matmul(w_real,x_data.T) + b_real\n",
    "\n",
    "y_data_pre_noise = sigmoid(wxb)\n",
    "y_data = np.random.binomial(1, y_data_pre_noise)\n",
    "y_pred = tf.sigmoid(y_pred)\n",
    "loss = y_true*tf.log(y_pred) - (1-y_true)*tf.log(1-y_pred)\n",
    "loss = tf.reduce_mean(loss)\n",
    "#tf.nn.sigmoid_cross_entropy_with_logits(labels=,logits=)\n",
    "NUM_STEPS = 50\n",
    "with tf.name_scope('loss') as scope:\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "# Before starting, initialize the variables. We will 'run' this first.\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(NUM_STEPS):\n",
    "        sess.run(train,{x: x_data, y_true: y_data})\n",
    "        if (step % 5 == 0):\n",
    "            print(step, sess.run([w,b]))\n",
    "            wb_.append(sess.run([w,b]))\n",
    "    print(50, sess.run([w,b]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
