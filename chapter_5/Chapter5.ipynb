{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text l : Working with Text and Sequences, and TensorBoard Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be covering recurrent neural network(RNNs) and long short-term memory(LSTM) networks and handle sequences of variable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind RNN models is that each new element in the\n",
    "sequence contributes some new information, which updates the current state of the current state of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental mathematical construct in statistics and probably, which is often \n",
    "used as building block for modelling sequential pattern via \n",
    "machine learning is the Markov chain model. We tend to view our\n",
    "data sequences as \"chains\", with each node in the chain dependent in some way on the\n",
    "previous node, so that \"history\" is not erased but carried on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN models are the based on this notion of chain structure. As the name\n",
    "implie, recurrent neural nets apply some form of \"loop.\" At some point in time t,\n",
    "the network observes an input x(t)(a word in a sentence) and update its \"state vector\" to h(t) from the \n",
    "previous vector h(t-1). When we process new input (the next word), it will be done in some manner that is dependent on h(t) and thus on\n",
    "the history of the sequence (the previous words we've seen affect our understanding of the current word).\n",
    "                             Recurrent structure can simply be viewed as one long unrolled chain, with each node in the chain performing the same \n",
    "                             kind of processing \"step\" based on the \"message\" it obtains from the output of the previous node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce some powerful, fairly low-level tools that Tensorflow provides for working\n",
    "with sequence data, which you can use to implement your own systems.\n",
    "We begin with our basic model mathematically. This mainly consists of defining the\n",
    "recurrence structure - the RNN update step.\n",
    "The update step for our simple vanilla RNN is\n",
    "h(t) = tanh(W(x)x(t) + W(h)h(t-1) + b)\n",
    "where W(h),W(x) and b are weight and bias variables. tanh(.) is the hyperbolic tangent function\n",
    "that has its range in [-1,1] and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
