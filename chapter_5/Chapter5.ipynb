{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text l : Working with Text and Sequences, and TensorBoard Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be covering recurrent neural network(RNNs) and long short-term memory(LSTM) networks and handle sequences of variable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind RNN models is that each new element in the\n",
    "sequence contributes some new information, which updates the current state of the current state of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental mathematical construct in statistics and probably, which is often \n",
    "used as building block for modelling sequential pattern via \n",
    "machine learning is the Markov chain model. We tend to view our\n",
    "data sequences as \"chains\", with each node in the chain dependent in some way on the\n",
    "previous node, so that \"history\" is not erased but carried on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN models are the based on this notion of chain structure. As the name\n",
    "implie, recurrent neural nets apply some form of \"loop.\" At some point in time t,\n",
    "the network observes an input x(t)(a word in a sentence) and update its \"state vector\" to h(t) from the \n",
    "previous vector h(t-1). When we process new input (the next word), it will be done in some manner that is dependent on h(t) and thus on\n",
    "the history of the sequence (the previous words we've seen affect our understanding of the current word).\n",
    "                             Recurrent structure can simply be viewed as one long unrolled chain, with each node in the chain performing the same \n",
    "                             kind of processing \"step\" based on the \"message\" it obtains from the output of the previous node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce some powerful, fairly low-level tools that Tensorflow provides for working\n",
    "with sequence data, which you can use to implement your own systems.\n",
    "We begin with our basic model mathematically. This mainly consists of defining the\n",
    "recurrence structure - the RNN update step.\n",
    "The update step for our simple vanilla RNN is\n",
    "h(t) = tanh(W(x)x(t) + W(h)h(t-1) + b)\n",
    "where W(h),W(x) and b are weight and bias variables. tanh(.) is the hyperbolic tangent function\n",
    "that has its range in [-1,1] and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST image as sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous chapter the architecture of convolutional neural networks makes\n",
    "use of the spatial structure of images, it is revealing to look at the structure of \n",
    "images from different angles by trying to capture in some sense the \"generative process\" that\n",
    "created each image. Intuitively, this all comes down to the notion that nearby areas in \n",
    "images are somehow related, and trying to model this structure.\n",
    "In our MNIST data, this just means that each 28 * 28 pixel image can be viewed as sequence of lengh 28,\n",
    "each element in the sequence a vector of 28 pixels. Then the temporal dependencies in the RNN can be imaged as a scanner \n",
    "head, scanning the image from top to buttom(rows) or left to right (columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading data, defining some parameters, and creating placeholders for\n",
    "our data:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)\n",
    "\n",
    "#Define some parameters\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# Where to save TensorBoard model summaries\n",
    "LOG_DIR = \"logs/RNN_with_summaries\"\n",
    "\n",
    "# Create placeholders for inputs, labels\n",
    "_inputs = tf.placeholder(tf.float32, shape=[None, time_steps, element_size], name=\"inputs\")\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name=\"labels\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
