{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text l : Working with Text and Sequences, and TensorBoard Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be covering recurrent neural network(RNNs) and long short-term memory(LSTM) networks and handle sequences of variable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind RNN models is that each new element in the\n",
    "sequence contributes some new information, which updates the current state of the current state of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental mathematical construct in statistics and probably, which is often \n",
    "used as building block for modelling sequential pattern via \n",
    "machine learning is the Markov chain model. We tend to view our\n",
    "data sequences as \"chains\", with each node in the chain dependent in some way on the\n",
    "previous node, so that \"history\" is not erased but carried on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN models are the based on this notion of chain structure. As the name\n",
    "implie, recurrent neural nets apply some form of \"loop.\" At some point in time t,\n",
    "the network observes an input x(t)(a word in a sentence) and update its \"state vector\" to h(t) from the \n",
    "previous vector h(t-1). When we process new input (the next word), it will be done in some manner that is dependent on h(t) and thus on\n",
    "the history of the sequence (the previous words we've seen affect our understanding of the current word).\n",
    "                             Recurrent structure can simply be viewed as one long unrolled chain, with each node in the chain performing the same \n",
    "                             kind of processing \"step\" based on the \"message\" it obtains from the output of the previous node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce some powerful, fairly low-level tools that Tensorflow provides for working\n",
    "with sequence data, which you can use to implement your own systems.\n",
    "We begin with our basic model mathematically. This mainly consists of defining the\n",
    "recurrence structure - the RNN update step.\n",
    "The update step for our simple vanilla RNN is\n",
    "h(t) = tanh(W(x)x(t) + W(h)h(t-1) + b)\n",
    "where W(h),W(x) and b are weight and bias variables. tanh(.) is the hyperbolic tangent function\n",
    "that has its range in [-1,1] and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST image as sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous chapter the architecture of convolutional neural networks makes\n",
    "use of the spatial structure of images, it is revealing to look at the structure of \n",
    "images from different angles by trying to capture in some sense the \"generative process\" that\n",
    "created each image. Intuitively, this all comes down to the notion that nearby areas in \n",
    "images are somehow related, and trying to model this structure.\n",
    "In our MNIST data, this just means that each 28 * 28 pixel image can be viewed as sequence of lengh 28,\n",
    "each element in the sequence a vector of 28 pixels. Then the temporal dependencies in the RNN can be imaged as a scanner \n",
    "head, scanning the image from top to buttom(rows) or left to right (columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading data, defining some parameters, and creating placeholders for\n",
    "our data:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)\n",
    "\n",
    "#Define some parameters\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# Where to save TensorBoard model summaries\n",
    "LOG_DIR = \"logs/RNN_with_summaries\"\n",
    "\n",
    "# Create placeholders for inputs, labels\n",
    "_inputs = tf.placeholder(tf.float32, shape=[None, time_steps, element_size], name=\"inputs\")\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name=\"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "# Reshape data to 28 sequence of 28 pixels\n",
    "batch_x = batch_x.reshape((batch_size, time_steps, element_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This helper function taken from official TensorFlow documentation, \n",
    "# simply add some ops that take care of logging summaries\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "      mean = tf.reduce_mean(var)\n",
    "      tf.summary.scalar('mean', mean)\n",
    "      with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "      tf.summary.scalar('stddev', stddev)\n",
    "      tf.summary.scalar('max', tf.reduce_max(var))\n",
    "      tf.summary.scalar('min', tf.reduce_min(var))\n",
    "      tf.summary.histogram('histogram', var)\n",
    "      \n",
    "\n",
    "# Weights and bias for input and hidden layer\n",
    "with tf.name_scope('rnn_weights'):\n",
    "        with tf.name_scope(\"W_x\"):\n",
    "            Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "            variable_summaries(Wx)\n",
    "        with tf.name_scope(\"W_h\"):\n",
    "            Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n",
    "            variable_summaries(Wh)\n",
    "        with tf.name_scope(\"Bias\"):\n",
    "            b_rnn = tf.Variable(tf.zeros([hidden_layer_size])) \n",
    "            variable_summaries(b_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_step(previous_hidden_state,x):\n",
    "    \n",
    "        current_hidden_state = tf.tanh(\n",
    "            tf.matmul(previous_hidden_state, Wh) +\n",
    "            tf.matmul(x, Wx) + b_rnn)\n",
    "\n",
    "        return current_hidden_state\n",
    "           \n",
    "# Processing inputs to work with scan function\n",
    "# Current input shape: (batch_size, time_steps, element_size)\n",
    "processed_input = tf.transpose(_inputs, perm=[1, 0, 2])\n",
    "# Current input shape now: (time_steps,batch_size, element_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "initial_hidden = tf.zeros([batch_size,hidden_layer_size])\n",
    "# Getting all state vectors across time\n",
    "all_hidden_states = tf.scan(rnn_step,\n",
    "                            processed_input,\n",
    "                            initializer=initial_hidden,\n",
    "                            name='states')\n",
    "\n",
    "\n",
    "# Weights for output layers\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    with tf.name_scope(\"W_linear\"):\n",
    "        Wl = tf.Variable(tf.truncated_normal([hidden_layer_size,\n",
    "                                              num_classes],\n",
    "                                              mean=0,stddev=.01))\n",
    "        variable_summaries(Wl)\n",
    "    with tf.name_scope(\"Bias_linear\"):\n",
    "        bl = tf.Variable(tf.truncated_normal([num_classes],\n",
    "                                             mean=0,stddev=.01))\n",
    "        variable_summaries(bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply linear layer to state vector    \n",
    "def get_linear_layer(hidden_state):\n",
    "\n",
    "    return tf.matmul(hidden_state, Wl) + bl\n",
    "\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    #Iterate across time, apply linear layer to all RNN outputs\n",
    "    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n",
    "    #Get Last output -- h_28\n",
    "    output = all_outputs[-1]\n",
    "    tf.summary.histogram('outputs', output)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    #Using RMSPropOptimizer\n",
    "    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))\n",
    "\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 2.303699, Training Accuracy= 8.59375\n",
      "Iter 1000, Minibatch Loss= 1.185607, Training Accuracy= 57.03125\n",
      "Iter 2000, Minibatch Loss= 0.484716, Training Accuracy= 82.81250\n",
      "Iter 3000, Minibatch Loss= 0.177755, Training Accuracy= 96.09375\n",
      "Iter 4000, Minibatch Loss= 0.114420, Training Accuracy= 97.65625\n",
      "Iter 5000, Minibatch Loss= 0.098224, Training Accuracy= 96.87500\n",
      "Iter 6000, Minibatch Loss= 0.027166, Training Accuracy= 99.21875\n",
      "Iter 7000, Minibatch Loss= 0.159078, Training Accuracy= 96.09375\n",
      "Iter 8000, Minibatch Loss= 0.086958, Training Accuracy= 96.87500\n",
      "Iter 9000, Minibatch Loss= 0.156590, Training Accuracy= 96.87500\n",
      "Test Accuracy: 97.6563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#Get a small test set  \n",
    "test_data = mnist.test.images[:batch_size].reshape((-1, time_steps,\n",
    "                                                     element_size))\n",
    "test_label = mnist.test.labels[:batch_size]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Write summaries to LOG_DIR -- used by TensorBoard\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train',\n",
    "                                         graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + '/test',\n",
    "                                        graph=tf.get_default_graph())\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(10000):\n",
    "        \n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Reshape data to get 28 sequences of 28 pixels\n",
    "            batch_x = batch_x.reshape((batch_size, time_steps,\n",
    "                                       element_size))\n",
    "            summary,_ =sess.run([merged,train_step],\n",
    "                                feed_dict={_inputs:batch_x, y:batch_y})\n",
    "            #Add to summaries\n",
    "            train_writer.add_summary(summary, i)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                acc,loss, = sess.run([accuracy,cross_entropy],\n",
    "                                     feed_dict={_inputs: batch_x,\n",
    "                                                y: batch_y})\n",
    "                print (\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))   \n",
    "            if i % 100 == 0:\n",
    "                # Calculate accuracy for 128 mnist test images and\n",
    "                #add to summaries\n",
    "                summary, acc = sess.run([merged, accuracy],\n",
    "                                        feed_dict={_inputs: test_data,\n",
    "                                                   y: test_label})\n",
    "                test_writer.add_summary(summary, i)\n",
    "\n",
    "    test_acc = sess.run(accuracy, feed_dict={_inputs: test_data,\n",
    "                                             y: test_label})\n",
    "    print (\"Test Accuracy:\", test_acc)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Visualizing the model with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is an interactive browser-based tool that allows us to visualize the learning process.\n",
    "To run TensorBoard, go to the command terminal and tell TensorBoard where the relevant summaries you logged are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-8-4a0fb5f12a4e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-4a0fb5f12a4e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tensorboard --logdir=LOG_DIR\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "If you are on Windows use:tensorboard --logdir=rnn_demo:LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard allows us to assign names to individual log directories by putting a colon between the name and the path, which may\n",
    "be useful when working with multiple log directories. In such a case, we pass a comma-seperated list of log directories as follows-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard --logdir=rnn_demo1:LOG_DIR1, rnn_demo2:LOG_DIR2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the tensorboard, go to the directory containing the log and run the tensorboard command in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Starting TensorBoard b'39' on port 6006\n",
    "#(You can navigate to http://10.100.102.4:6006)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
