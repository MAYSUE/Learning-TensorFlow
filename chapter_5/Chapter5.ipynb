{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text l : Working with Text and Sequences, and TensorBoard Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be covering recurrent neural network(RNNs) and long short-term memory(LSTM) networks and handle sequences of variable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind RNN models is that each new element in the\n",
    "sequence contributes some new information, which updates the current state of the current state of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental mathematical construct in statistics and probably, which is often \n",
    "used as building block for modelling sequential pattern via \n",
    "machine learning is the Markov chain model. We tend to view our\n",
    "data sequences as \"chains\", with each node in the chain dependent in some way on the\n",
    "previous node, so that \"history\" is not erased but carried on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN models are the based on this notion of chain structure. As the name\n",
    "implie, recurrent neural nets apply some form of \"loop.\" At some point in time t,\n",
    "the network observes an input x(t)(a word in a sentence) and update its \"state vector\" to h(t) from the \n",
    "previous vector h(t-1). When we process new input (the next word), it will be done in some manner that is dependent on h(t) and thus on\n",
    "the history of the sequence (the previous words we've seen affect our understanding of the current word).\n",
    "                             Recurrent structure can simply be viewed as one long unrolled chain, with each node in the chain performing the same \n",
    "                             kind of processing \"step\" based on the \"message\" it obtains from the output of the previous node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce some powerful, fairly low-level tools that Tensorflow provides for working\n",
    "with sequence data, which you can use to implement your own systems.\n",
    "We begin with our basic model mathematically. This mainly consists of defining the\n",
    "recurrence structure - the RNN update step.\n",
    "The update step for our simple vanilla RNN is\n",
    "h(t) = tanh(W(x)x(t) + W(h)h(t-1) + b)\n",
    "where W(h),W(x) and b are weight and bias variables. tanh(.) is the hyperbolic tangent function\n",
    "that has its range in [-1,1] and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST image as sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous chapter the architecture of convolutional neural networks makes\n",
    "use of the spatial structure of images, it is revealing to look at the structure of \n",
    "images from different angles by trying to capture in some sense the \"generative process\" that\n",
    "created each image. Intuitively, this all comes down to the notion that nearby areas in \n",
    "images are somehow related, and trying to model this structure.\n",
    "In our MNIST data, this just means that each 28 * 28 pixel image can be viewed as sequence of lengh 28,\n",
    "each element in the sequence a vector of 28 pixels. Then the temporal dependencies in the RNN can be imaged as a scanner \n",
    "head, scanning the image from top to buttom(rows) or left to right (columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading data, defining some parameters, and creating placeholders for\n",
    "our data:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)\n",
    "\n",
    "#Define some parameters\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# Where to save TensorBoard model summaries\n",
    "LOG_DIR = \"logs/RNN_with_summaries\"\n",
    "\n",
    "# Create placeholders for inputs, labels\n",
    "_inputs = tf.placeholder(tf.float32, shape=[None, time_steps, element_size], name=\"inputs\")\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name=\"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "# Reshape data to 28 sequence of 28 pixels\n",
    "batch_x = batch_x.reshape((batch_size, time_steps, element_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This helper function taken from official TensorFlow documentation, \n",
    "# simply add some ops that take care of logging summaries\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "      mean = tf.reduce_mean(var)\n",
    "      tf.summary.scalar('mean', mean)\n",
    "      with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "      tf.summary.scalar('stddev', stddev)\n",
    "      tf.summary.scalar('max', tf.reduce_max(var))\n",
    "      tf.summary.scalar('min', tf.reduce_min(var))\n",
    "      tf.summary.histogram('histogram', var)\n",
    "      \n",
    "\n",
    "# Weights and bias for input and hidden layer\n",
    "with tf.name_scope('rnn_weights'):\n",
    "        with tf.name_scope(\"W_x\"):\n",
    "            Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "            variable_summaries(Wx)\n",
    "        with tf.name_scope(\"W_h\"):\n",
    "            Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n",
    "            variable_summaries(Wh)\n",
    "        with tf.name_scope(\"Bias\"):\n",
    "            b_rnn = tf.Variable(tf.zeros([hidden_layer_size])) \n",
    "            variable_summaries(b_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_step(previous_hidden_state,x):\n",
    "    \n",
    "        current_hidden_state = tf.tanh(\n",
    "            tf.matmul(previous_hidden_state, Wh) +\n",
    "            tf.matmul(x, Wx) + b_rnn)\n",
    "\n",
    "        return current_hidden_state\n",
    "           \n",
    "# Processing inputs to work with scan function\n",
    "# Current input shape: (batch_size, time_steps, element_size)\n",
    "processed_input = tf.transpose(_inputs, perm=[1, 0, 2])\n",
    "# Current input shape now: (time_steps,batch_size, element_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "initial_hidden = tf.zeros([batch_size,hidden_layer_size])\n",
    "# Getting all state vectors across time\n",
    "all_hidden_states = tf.scan(rnn_step,\n",
    "                            processed_input,\n",
    "                            initializer=initial_hidden,\n",
    "                            name='states')\n",
    "\n",
    "\n",
    "# Weights for output layers\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    with tf.name_scope(\"W_linear\"):\n",
    "        Wl = tf.Variable(tf.truncated_normal([hidden_layer_size,\n",
    "                                              num_classes],\n",
    "                                              mean=0,stddev=.01))\n",
    "        variable_summaries(Wl)\n",
    "    with tf.name_scope(\"Bias_linear\"):\n",
    "        bl = tf.Variable(tf.truncated_normal([num_classes],\n",
    "                                             mean=0,stddev=.01))\n",
    "        variable_summaries(bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply linear layer to state vector    \n",
    "def get_linear_layer(hidden_state):\n",
    "\n",
    "    return tf.matmul(hidden_state, Wl) + bl\n",
    "\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    #Iterate across time, apply linear layer to all RNN outputs\n",
    "    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n",
    "    #Get Last output -- h_28\n",
    "    output = all_outputs[-1]\n",
    "    tf.summary.histogram('outputs', output)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    #Using RMSPropOptimizer\n",
    "    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))\n",
    "\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 2.303699, Training Accuracy= 8.59375\n",
      "Iter 1000, Minibatch Loss= 1.185607, Training Accuracy= 57.03125\n",
      "Iter 2000, Minibatch Loss= 0.484716, Training Accuracy= 82.81250\n",
      "Iter 3000, Minibatch Loss= 0.177755, Training Accuracy= 96.09375\n",
      "Iter 4000, Minibatch Loss= 0.114420, Training Accuracy= 97.65625\n",
      "Iter 5000, Minibatch Loss= 0.098224, Training Accuracy= 96.87500\n",
      "Iter 6000, Minibatch Loss= 0.027166, Training Accuracy= 99.21875\n",
      "Iter 7000, Minibatch Loss= 0.159078, Training Accuracy= 96.09375\n",
      "Iter 8000, Minibatch Loss= 0.086958, Training Accuracy= 96.87500\n",
      "Iter 9000, Minibatch Loss= 0.156590, Training Accuracy= 96.87500\n",
      "Test Accuracy: 97.6563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#Get a small test set  \n",
    "test_data = mnist.test.images[:batch_size].reshape((-1, time_steps,\n",
    "                                                     element_size))\n",
    "test_label = mnist.test.labels[:batch_size]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Write summaries to LOG_DIR -- used by TensorBoard\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train',\n",
    "                                         graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + '/test',\n",
    "                                        graph=tf.get_default_graph())\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(10000):\n",
    "        \n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Reshape data to get 28 sequences of 28 pixels\n",
    "            batch_x = batch_x.reshape((batch_size, time_steps,\n",
    "                                       element_size))\n",
    "            summary,_ =sess.run([merged,train_step],\n",
    "                                feed_dict={_inputs:batch_x, y:batch_y})\n",
    "            #Add to summaries\n",
    "            train_writer.add_summary(summary, i)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                acc,loss, = sess.run([accuracy,cross_entropy],\n",
    "                                     feed_dict={_inputs: batch_x,\n",
    "                                                y: batch_y})\n",
    "                print (\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))   \n",
    "            if i % 100 == 0:\n",
    "                # Calculate accuracy for 128 mnist test images and\n",
    "                #add to summaries\n",
    "                summary, acc = sess.run([merged, accuracy],\n",
    "                                        feed_dict={_inputs: test_data,\n",
    "                                                   y: test_label})\n",
    "                test_writer.add_summary(summary, i)\n",
    "\n",
    "    test_acc = sess.run(accuracy, feed_dict={_inputs: test_data,\n",
    "                                             y: test_label})\n",
    "    print (\"Test Accuracy:\", test_acc)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Visualizing the model with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is an interactive browser-based tool that allows us to visualize the learning process.\n",
    "To run TensorBoard, go to the command terminal and tell TensorBoard where the relevant summaries you logged are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "If you are on Windows use:tensorboard --logdir=rnn_demo:LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard allows us to assign names to individual log directories by putting a colon between the name and the path, which may\n",
    "be useful when working with multiple log directories. In such a case, we pass a comma-seperated list of log directories as follows-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard --logdir=rnn_demo1:LOG_DIR1, rnn_demo2:LOG_DIR2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the tensorboard, go to the directory containing the log and run the tensorboard command in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Starting TensorBoard b'39' on port 6006\n",
    "#(You can navigate to http://10.100.102.4:6006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Built-in RNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/basic_rnn_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a4a3a6bbd7a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# TensorFlow built-in functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrnn_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m Wl = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes],\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    711\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[0;32m   2603\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2604\u001b[0m     \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2605\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2606\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2436\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2437\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 2438\u001b[1;33m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   2439\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2440\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2386\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2387\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m-> 2388\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2389\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2390\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    696\u001b[0m           skip_conditionals=True)\n\u001b[0;32m    697\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[1;31m# Pack state if using state tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"basic_rnn_cell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m       output = self._activation(\n\u001b[1;32m---> 64\u001b[1;33m           _linear([inputs, state], self._num_units, True, scope=scope))\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m_linear\u001b[1;34m(args, output_size, bias, bias_start, scope)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mouter_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     weights = vs.get_variable(\n\u001b[1;32m--> 747\u001b[1;33m         \"weights\", [total_arg_size, output_size], dtype=dtype)\n\u001b[0m\u001b[0;32m    748\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m       \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m       custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m           custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m           validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[0;32m    637\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 639\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable rnn/basic_rnn_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\George\\Anaconda3\\envs\\tenorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "element_size = 28; time_steps= 28; num_classes =10\n",
    "batch_size = 128; hidden_layer_size = 128\n",
    "\n",
    "_inputs = tf.placeholder(tf.float32,shape=[None, time_steps,\n",
    "element_size],\n",
    "name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes],name='inputs')\n",
    "\n",
    "# TensorFlow built-in functions\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(hidden_layer_size)\n",
    "outputs, _ = tf.nn.dynamic_rnn(rnn_cell, _inputs, dtype=tf.float32)\n",
    "\n",
    "Wl = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes],\n",
    "mean=0,stddev=.01))\n",
    "bl = tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))\n",
    "\n",
    "\n",
    "\n",
    "def get_linear_layer(vector):\n",
    "    return tf.matmul(vector, Wl) + bl\n",
    "\n",
    "last_rnn_output = outputs[:,-1,:]\n",
    "final_output = get_linear_layer(last_rnn_output)\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output,\n",
    "labels=y)\n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "test_data = mnist.test.images[:batch_size].reshape((-1,time_steps, element_size))\n",
    "test_label = mnist.test.labels[:batch_size]\n",
    "\n",
    "\n",
    "for i in range(3001):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n",
    "        sess.run(train_step,feed_dict={_inputs:batch_x,y:batch_y})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "                acc = sess.run(accuracy, feed_dict={_inputs: batch_x,y: batch_y})\n",
    "                loss = sess.run(cross_entropy,feed_dict={_inputs:batch_x,y:batch_y})\n",
    "                print(\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.5f}\".format(acc))\n",
    "print(\"Testing Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={_inputs: test_data, y: test_label}))\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
